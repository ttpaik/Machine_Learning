{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "EC414_HW3-Solutions.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARWoDwBSAeQz"
      },
      "source": [
        "# Homework 3: Optimization, KNN and Decision Trees\n",
        "by Rachel Manzelli and Brian Kulis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59YQA_pBAeQ4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rRL9a-2AeQ5"
      },
      "source": [
        "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\n",
        "\n",
        "To use Google Colab:\n",
        "\n",
        "1. Download this `ipynb` file.\n",
        "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\n",
        "3. Upload this file. It will then open in Colab.\n",
        "\n",
        "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\n",
        "\n",
        "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\n",
        "\n",
        "**Submission instructions**: please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by **February 24th at midnight**.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHqdYy5UAeQ5"
      },
      "source": [
        "### 1. Maximum Likelihood without a Closed-Form Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G43g2dyAeQ5"
      },
      "source": [
        "Assume that we are given $n$ IID samples ${x_1,...x_n}$ from the following $P(X|\\theta)$:\n",
        "\n",
        "$$P(X|\\theta) = \\frac{1}{\\pi}\\bigg [\\frac{1}{(x-\\theta)^2+1}\\bigg ]$$\n",
        "\n",
        "#### 1.a. Try to compute the MLE via maximizing the log-likelihood function directly, and briefly explain why this won't work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOX-mJhrAeQ6"
      },
      "source": [
        "#### Solution\n",
        "\n",
        "This is the Cauchy distribution, with $\\gamma = 1$. Following the steps to obtain the ML estimate for $\\theta$:\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} P(x_1,...x_n|\\theta)$$\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{n}P(x_i|\\theta) \\mbox{ (since $x_i$ are IID)}$$\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{n}\\bigg (\\frac{1}{\\pi}\\bigg [\\frac{1}{(x_i-\\theta)^2+1}\\bigg ]\\bigg ) \\mbox{ This is the likelihood function.}$$\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\log\\bigg (\\prod_{i=1}^{n}\\bigg (\\frac{1}{\\pi}\\bigg [\\frac{1}{(x_i-\\theta)^2+1}\\bigg ]\\bigg )\\bigg ) \\mbox{(take the log to obtain the log-likelihood function)}$$\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\log\\prod_{i=1}^{n}\\bigg (\\frac{1}{\\pi}\\bigg ) + \\log\\prod_{i=1}^{n}\\bigg (\\bigg [\\frac{1}{(x_i-\\theta)^2+1}\\bigg ]\\bigg )\\bigg ) \\mbox{ (log(xy) = log(x) + log(y))}$$\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} -\\log\\prod_{i=1}^{n}(\\pi) - \\log\\prod_{i=1}^{n}((x_i-\\theta)^2+1) \\mbox{ (flipped the fraction and negated the log)}$$\n",
        "\n",
        "$$MLE(\\theta) = \\underset{\\theta}{\\operatorname{argmax}} -n\\log\\pi - \\sum_{i=1}^{n}\\log((x_i-\\theta)^2+1) \\mbox{ (log product rule) This is the log-likelihood function.}$$\n",
        "\n",
        "$$\\frac{d}{d\\theta}\\bigg [-nlog\\pi - \\sum_{i=1}^{n}log((x_i-\\theta)^2+1)\\bigg ] = 0 \\mbox{ (take the derivative w.r.t $\\theta$ and set to 0)}$$\n",
        "\n",
        "$$\\frac{d}{d\\theta}\\bigg [-\\sum_{i=1}^{n}log((x_i-\\theta)^2+1)\\bigg ] = 0 \\mbox{ (remove terms not dependent on $\\theta$)}$$\n",
        "\n",
        "$$\\sum_{i=1}^{n}\\frac{2(x_i-\\theta)}{(x_i-\\theta)^2+1} = 0 \\mbox{ (simplify)}$$\n",
        "\n",
        "$$\\frac{2(x_1-\\theta)}{(x_1-\\theta)^2+1} + \\frac{2(x_2-\\theta)}{(x_2-\\theta)^2+1} + ... + \\frac{2(x_n-\\theta)}{(x_n-\\theta)^2+1} = 0$$\n",
        "\n",
        "It would be very hard to solve for $\\theta$ directly here, since we'd have to find the root of a polynomial with degree $2n-1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_odVBY-AAeQ6"
      },
      "source": [
        "#### 1.b. Convert the objective (log-likelihood) function into a cost function, $J(\\theta)$, that we can minimize using gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiAcxLLyAeQ6"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "$LL(\\theta) = -nlog\\pi - \\sum_{i=1}^{n}\\log((x_i-\\theta)^2+1)$ (we'd like to maximize this)\n",
        "\n",
        "$J(\\theta) = -LL(\\theta) = -[-n\\log\\pi - \\sum_{i=1}^{n}\\log((x_i-\\theta)^2+1)]$ (we'd like to minimize this, averaged over all training examples)\n",
        "\n",
        "$J(\\theta) = \\log\\pi + \\frac{1}{n}[\\sum_{i=1}^{n}\\log((x_i-\\theta)^2+1)]$ (simplify)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eDazGTLAeQ7"
      },
      "source": [
        "#### 1.c. Compute the gradient descent update rule, where $\\theta_{n+1}=\\theta_n - \\alpha\\frac{d}{d\\theta_n}J(\\theta_n)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR04U3GLAeQ7"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "$$\\theta_{n+1}=\\theta_n - \\alpha*\\frac{1}{n}\\sum_{i=1}^{n}\\bigg [\\frac{2(\\theta_n-x_i)}{(x_i-\\theta_n)^2+1}\\bigg ]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P2TAumrAeQ7"
      },
      "source": [
        "#### 1.d. Write the pseudocode for gradient descent with this update rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrlMwR9dAeQ7"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "`for i in num_iters, or repeat until convergence:`\n",
        "\n",
        "$$\\theta_{n+1}=\\theta_n - \\alpha*\\frac{1}{n}\\sum_{i=1}^{n}\\bigg [\\frac{2(\\theta_n-x_i)}{(x_i-\\theta_n)^2+1}\\bigg ]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1loAwOr-AeQ8"
      },
      "source": [
        "#### 1.e. (Bonus) Compute the stochastic gradient descent (SGD) update rule, and write pseudocode for SGD with this update rule. (Assume a mini-batch size of 1.)\n",
        "\n",
        "**Solution: SGD update rule**\n",
        "\n",
        "$$\\theta_{n+1}=\\theta_n - \\alpha\\bigg [\\frac{2(\\theta_n-x)}{(x-\\theta_n)^2+1}\\bigg ]$$\n",
        "\n",
        "**Solution: SGD pseudocode (batch size 1)**\n",
        "\n",
        "`for i in num_iters, or repeat until convergence:`\n",
        "\n",
        ">`for x in X_train:`\n",
        "    \n",
        "$$\\theta_{n+1}=\\theta_n - \\alpha\\bigg [\\frac{2(\\theta_n-x)}{(x-\\theta_n)^2+1}\\bigg ]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9vXoAu5AeQ8"
      },
      "source": [
        "### 2. Decision Trees\n",
        "\n",
        "The following dataset contains information about different weather attributes, along with whether the golf team decided to play. Each row represents the characteristics of one day.\n",
        "\n",
        "\n",
        "| Temp        | Humidity    | Wind        | Play?       |\n",
        "| ----------- | ----------- | ----------- | ----------- |\n",
        "| hot         | normal      | strong      | no          |\n",
        "| mild        | high        | strong      | yes         |\n",
        "| hot         | normal      | strong      | no          |\n",
        "| hot         | normal      | weak        | yes         |\n",
        "| mild        | normal      | strong      | yes         |\n",
        "\n",
        "\n",
        "We will construct a decision tree that predicts whether or not the current weather attributes are appropriate for playing golf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-0rI3CiAeQ8"
      },
      "source": [
        "#### 2.a. Choose a root node. \n",
        "Follow the method of using information gain to choose a root node for our decision tree, as described in class (and posted in the slides on Blackboard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErNisx31AeQ9"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "The root node is the node with the highest information gain. We'll calculate this by the following formula:\n",
        "\n",
        "$Gain(S,F) = Entropy(S) - \\sum_{v\\in values(F)}\\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropy(S_v)$, where $S_v$ is the subset of $S$ having value $v$ for feature $F$, and $Entropy = -p_1\\log_2 p_1 - p_0\\log_2 p_0$, where $p_1$ is the fraction of positive examples in the subset, and $p_0$ is that of negative examples.\n",
        "\n",
        "***Total dataset entropy, $Entropy(S)$***\n",
        "\n",
        "$Entropy(S) = -P(yes)\\log_{2} P(yes) - P(no)\\log_{2} P(no) = -\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5} = 0.971$\n",
        "\n",
        "***Temperature***\n",
        "\n",
        "$Gain(S, T) = Entropy(S) - \\sum_{v\\in values(T)}\\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropy(S_v)$\n",
        "\n",
        "$Gain(S, T) = Entropy(S) - \\bigg [\\frac{\\left|S_{hot}\\right|}{\\left|S\\right|}Entropy(S_{hot}) + \\frac{\\left|S_{mild}\\right|}{\\left|S\\right|}Entropy(S_{mild})\\bigg ]$\n",
        "\n",
        "$Gain(S, T) = 0.971 - \\bigg [\\frac{3}{5}(-\\frac{1}{3}\\log_2\\frac{1}{3} - \\frac{2}{3}\\log_2\\frac{2}{3}) + \\frac{2}{5}(-\\frac{0}{2}\\log_2\\frac{0}{2} - \\frac{2}{2}\\log_2\\frac{2}{2})\\bigg ]$\n",
        "\n",
        "$Gain(S, T) = 0.971 - \\bigg [\\frac{3}{5}(0.918) + \\frac{2}{5}(0)\\bigg ] = 0.971 - 0.551$\n",
        "\n",
        "$Gain(S, T) = 0.42$\n",
        "\n",
        "***Humidity***\n",
        "\n",
        "$Gain(S, H) = Entropy(S) - \\sum_{v\\in values(H)}\\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropy(S_v)$\n",
        "\n",
        "$Gain(S, H) = Entropy(S) - \\bigg [\\frac{\\left|S_{high}\\right|}{\\left|S\\right|}Entropy(S_{high}) + \\frac{\\left|S_{normal}\\right|}{\\left|S\\right|}Entropy(S_{normal})\\bigg ]$\n",
        "\n",
        "$Gain(S, H) = 0.971 - \\bigg [\\frac{1}{5}(-\\frac{1}{1}\\log_2\\frac{1}{1} - \\frac{0}{1}\\log_2\\frac{0}{1}) + \\frac{4}{5}(-\\frac{2}{4}\\log_2\\frac{2}{4} - \\frac{2}{4}\\log_2\\frac{2}{4})\\bigg ]$\n",
        "\n",
        "$Gain(S, H) = 0.971 - \\bigg [\\frac{1}{5}(0) + \\frac{4}{5}(1)\\bigg ] = 0.971 - 0.8$\n",
        "\n",
        "$Gain(S, H) = 0.171$\n",
        "\n",
        "***Wind***\n",
        "\n",
        "$Gain(S, W) = Entropy(S) - \\sum_{v\\in values(W)}\\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropy(S_v)$\n",
        "\n",
        "$Gain(S, W) = Entropy(S) - \\bigg [\\frac{\\left|S_{strong}\\right|}{\\left|S\\right|}Entropy(S_{strong}) + \\frac{\\left|S_{weak}\\right|}{\\left|S\\right|}Entropy(S_{weak})\\bigg ]$\n",
        "\n",
        "$Gain(S, W) = 0.971 - \\bigg [\\frac{4}{5}(-\\frac{2}{4}\\log_2\\frac{2}{4} - \\frac{2}{4}\\log_2\\frac{2}{4}) + \\frac{1}{5}(-\\frac{1}{1}\\log_2\\frac{1}{1} - \\frac{0}{1}\\log_2\\frac{0}{1})\\bigg ]$\n",
        "\n",
        "$Gain(S, W) = 0.971 - \\bigg [\\frac{4}{5}(1) + \\frac{1}{5}(0)\\bigg ] = 0.971 - 0.8$\n",
        "\n",
        "$Gain(S, W) = 0.171$\n",
        "\n",
        "Since temperature has the highest information gain, we'll use **temperature as the root node** to make our first split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYRQgIK1AeQ-"
      },
      "source": [
        "#### 2.b. Complete the tree.\n",
        "Repeat the method of using information gain to split on another feature, as described in class (and posted in the slides on Blackboard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrnCTvZ5AeQ-"
      },
      "source": [
        "Since our decision tree will always decide to play golf when the temperature is `mild`, we'll remove those samples from our dataset for our next split. We're left with the following:\n",
        "\n",
        "| Temp        | Humidity    | Wind        | Play?       |\n",
        "| ----------- | ----------- | ----------- | ----------- |\n",
        "| hot         | normal      | strong      | no          |\n",
        "| hot         | normal      | strong      | no          |\n",
        "| hot         | normal      | weak        | yes         |\n",
        "\n",
        "We'll choose the next (and final) feature by repeating our search for the highest information gain, adjusting our calculations to only include the remaining samples.\n",
        "\n",
        "***Total dataset entropy, $Entropy(S)$***\n",
        "\n",
        "$Entropy(S) = -P(yes)\\log_{2} P(yes) - P(no)\\log_{2} P(no) = -\\frac{1}{3}\\log_{2}\\frac{1}{3} - \\frac{2}{3}\\log_{2}\\frac{2}{3} = 0.918$\n",
        "\n",
        "***Humidity***\n",
        "\n",
        "$Gain(S, H) = Entropy(S) - \\sum_{v\\in values(H)}\\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropy(S_v)$\n",
        "\n",
        "$Gain(S, H) = Entropy(S) - \\bigg [\\frac{\\left|S_{high}\\right|}{\\left|S\\right|}Entropy(S_{high}) + \\frac{\\left|S_{normal}\\right|}{\\left|S\\right|}Entropy(S_{normal})\\bigg ]$\n",
        "\n",
        "$Gain(S, H) = 0.918 - \\bigg [0 + \\frac{3}{3}(-\\frac{2}{3}\\log_2\\frac{2}{3} - \\frac{1}{3}\\log_2\\frac{1}{3})\\bigg ]$\n",
        "\n",
        "$Gain(S, H) = 0.918 - 0.918$\n",
        "\n",
        "$Gain(S, H) = 0$\n",
        "\n",
        "***Wind***\n",
        "\n",
        "$Gain(S, W) = Entropy(S) - \\sum_{v\\in values(W)}\\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropy(S_v)$\n",
        "\n",
        "$Gain(S, W) = Entropy(S) - \\bigg [\\frac{\\left|S_{strong}\\right|}{\\left|S\\right|}Entropy(S_{strong}) + \\frac{\\left|S_{weak}\\right|}{\\left|S\\right|}Entropy(S_{weak})\\bigg ]$\n",
        "\n",
        "$Gain(S, W) = 0.918 - \\bigg [\\frac{2}{3}(-\\frac{0}{2}\\log_2\\frac{0}{2} - \\frac{2}{2}\\log_2\\frac{2}{2}) + \\frac{1}{3}(-\\frac{1}{1}\\log_2\\frac{1}{1} - \\frac{0}{1}\\log_2\\frac{0}{1})\\bigg ]$\n",
        "\n",
        "$Gain(S, W) = 0.918 - \\bigg [\\frac{2}{3}(0) + \\frac{1}{3}(0)\\bigg ] = 0.918 - 0$\n",
        "\n",
        "$Gain(S, W) = 0.918$\n",
        "\n",
        "Wind has the highest information gain, so we'll now **split on wind**.\n",
        "\n",
        "***Final tree:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntAc1r-lAeQ_"
      },
      "source": [
        "![Final Tree](decision_tree_golf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWBwMo-6AeQ_"
      },
      "source": [
        "### 3. K-Nearest Neighbors on NIST\n",
        "\n",
        "We're going to build a K-nearest neighbors classifier from scratch, including validating for the best K, and test it out on NIST, a handwritten digits dataset.\n",
        "\n",
        "The 64 features of this dataset are the values of each pixel in the 8x8 image grid of one handwritten digit, where the digits are written in white (pixel value 255) and the surrounding space is black (pixel value 0). Each sample represents one image.\n",
        "\n",
        "#### First, install the latest release of `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOXz07w8AeRA",
        "outputId": "47aaf8f6-b4fa-4709-e3a3-c99ce2799cc3"
      },
      "source": [
        "# These datasets require the latest release of sklearn (0.24.1).\n",
        "# We are going to uninstall the default Colab version (if you are using Colab) or your current version, and install version 0.24.1.\n",
        "# AFTER RUNNING THIS CELL, YOU WILL NEED TO RESTART THE RUNTIME. GO TO Runtime/Kernel -> Restart Runtime to do this. \n",
        "# (Or, in Colab, hit the RESTART RUNTIME button at the bottom of the error message when you run this cell.)\n",
        "# You only need to do this once, but if the Colab runtime disconnects, you will need to do it again!\n",
        "\n",
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==0.24.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: scikit-learn 0.23.1\n",
            "Uninstalling scikit-learn-0.23.1:\n",
            "  Successfully uninstalled scikit-learn-0.23.1\n",
            "Collecting scikit-learn==0.24.1\n",
            "  Using cached scikit_learn-0.24.1-cp37-cp37m-macosx_10_13_x86_64.whl (7.2 MB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn==0.24.1) (1.19.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn==0.24.1) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn==0.24.1) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn==0.24.1) (1.0.0)\n",
            "Installing collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-0.24.1\n",
            "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfnPrnVUAeRB"
      },
      "source": [
        "#### Import the data and take a look at some samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enOa05GiAeRB",
        "outputId": "aa299bf8-e6f8-4c4a-d054-a23f8d986ae5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Import the data into a pandas dataframe\n",
        "nist = load_digits()\n",
        "nist_df = pd.DataFrame(nist.data, columns = nist.feature_names)\n",
        "\n",
        "# Split data into features and labels\n",
        "X = nist.data\n",
        "y = nist.target\n",
        "\n",
        "# View the raw data\n",
        "nist_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel_0_0</th>\n",
              "      <th>pixel_0_1</th>\n",
              "      <th>pixel_0_2</th>\n",
              "      <th>pixel_0_3</th>\n",
              "      <th>pixel_0_4</th>\n",
              "      <th>pixel_0_5</th>\n",
              "      <th>pixel_0_6</th>\n",
              "      <th>pixel_0_7</th>\n",
              "      <th>pixel_1_0</th>\n",
              "      <th>pixel_1_1</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel_6_6</th>\n",
              "      <th>pixel_6_7</th>\n",
              "      <th>pixel_7_0</th>\n",
              "      <th>pixel_7_1</th>\n",
              "      <th>pixel_7_2</th>\n",
              "      <th>pixel_7_3</th>\n",
              "      <th>pixel_7_4</th>\n",
              "      <th>pixel_7_5</th>\n",
              "      <th>pixel_7_6</th>\n",
              "      <th>pixel_7_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 64 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
              "0        0.0        0.0        5.0       13.0        9.0        1.0   \n",
              "1        0.0        0.0        0.0       12.0       13.0        5.0   \n",
              "2        0.0        0.0        0.0        4.0       15.0       12.0   \n",
              "3        0.0        0.0        7.0       15.0       13.0        1.0   \n",
              "4        0.0        0.0        0.0        1.0       11.0        0.0   \n",
              "\n",
              "   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
              "0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "2        0.0        0.0        0.0        0.0  ...        5.0        0.0   \n",
              "3        0.0        0.0        0.0        8.0  ...        9.0        0.0   \n",
              "4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
              "\n",
              "   pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
              "0        0.0        0.0        6.0       13.0       10.0        0.0   \n",
              "1        0.0        0.0        0.0       11.0       16.0       10.0   \n",
              "2        0.0        0.0        0.0        3.0       11.0       16.0   \n",
              "3        0.0        0.0        7.0       13.0       13.0        9.0   \n",
              "4        0.0        0.0        0.0        2.0       16.0        4.0   \n",
              "\n",
              "   pixel_7_6  pixel_7_7  \n",
              "0        0.0        0.0  \n",
              "1        0.0        0.0  \n",
              "2        9.0        0.0  \n",
              "3        0.0        0.0  \n",
              "4        0.0        0.0  \n",
              "\n",
              "[5 rows x 64 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TTPJG2MAeRB",
        "outputId": "230aed50-1b65-4feb-b4a9-d3581b3e71ae"
      },
      "source": [
        "# Visualize 10 random samples as 8x8 images\n",
        "from matplotlib import pyplot as plt\n",
        "fig = plt.figure(figsize=(20, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(nist.images[i*4], cmap=plt.cm.binary, interpolation='nearest')\n",
        "    # Label the image with the target value\n",
        "    ax.text(0, 7, str(nist.target[i*4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3rGtmSNAeRC"
      },
      "source": [
        "#### 3.a. Split the data into train, test, and validation sets.\n",
        "\n",
        "Use a 60/20/20 split. Make sure to set `random_state=42` to shuffle the dataset in a consistent manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPmY5BQUAeRC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training, test, and validation sets, just like we did on HW 2\n",
        "# Make sure to set random_state=42!\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42, test_size=0.2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=True, random_state=42, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Trnv3NHAeRC"
      },
      "source": [
        "#### 3.b. Implement a Euclidean distance function.\n",
        "We'll need to calculate the Euclidean distance between points of arbitrary dimensions, which you'll implement by filling in the function below. (*Hint: use [this](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html).*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAJMc8AkAeRC"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def euclidean_distance(a, b):\n",
        "    '''\n",
        "    A function that calculates and returns the Euclidean distance between vectors a and b.\n",
        "    '''\n",
        "    dist = distance.euclidean(a, b)\n",
        "    return dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYYvBCYHAeRC"
      },
      "source": [
        "#### 3.c. Implement the K-nearest neighbors algorithm.\n",
        "The two functions below make up a skeleton of the KNN algorithm, which you will complete.\n",
        "\n",
        "The first, `compute_neighbors_and_classify()`:\n",
        "1. Computes the neighbors given `k` (the number of neighbors)\n",
        "2. Finds the `k` neighbors with the smallest Euclidean distance to the test point (using `euclidean_distance()` above)\n",
        "3. Returns the most common classification label among those `k` neighbors.\n",
        "\n",
        "The second, `compute_accuracy_on_dataset()`:\n",
        "1. Gets the KNN classification predictions for each test point using `compute_neighbors_and_classify()`.\n",
        "2. Compares the predictions to the real values.\n",
        "3. Returns the accuracy score (how many predictions the model got right divided by total number of samples)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fpcb5qoAeRC"
      },
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "def compute_neighbors_and_classify(X_train, y_train, test_point, k):\n",
        "    '''\n",
        "    A function that computes the k neighbors in X_train closest to one test point, \n",
        "    and returns the classification.\n",
        "    '''\n",
        "    # This is a list to hold all of the distances associated with the points and their labels\n",
        "    distances = []\n",
        "    # Loop over the training points\n",
        "    for i, train_point in enumerate(X_train):\n",
        "        # Compute the Euclidean distance `dist` from this training point to the test point\n",
        "        dist = euclidean_distance(train_point, test_point)\n",
        "        # Add the distance, the point, and its label (as a tuple) value to our list\n",
        "        # We use a tuple so we can sort the list later while keeping each neighbor next to its distance\n",
        "        distances.append((dist, train_point, y_train[i]))\n",
        "\n",
        "    # Now that we have all the distances, we need to return the labels with the k smallest distances\n",
        "    # First, sort the list we made by distance\n",
        "    distances = sorted(distances, key=lambda x: x[0])\n",
        "    \n",
        "    # Now, pull out the labels associated with the first k neighbors and add them to a list\n",
        "    k_labels = []\n",
        "    for i in range(k):\n",
        "        k_labels.append(distances[i][2])\n",
        "    \n",
        "    # Get the label that appears the most times in k_labels using mode(). This is our `classification`.\n",
        "    # If there is no mode, just return any label within the k_labels list.\n",
        "    classification = mode(k_labels)[0]\n",
        "    if not classification:\n",
        "        return k_labels[0]\n",
        "    else:\n",
        "        return classification\n",
        "\n",
        "\n",
        "def compute_accuracy_on_dataset(X_train, y_train, data, labels, k):\n",
        "    '''\n",
        "    A function that computes the accuracy of KNN on a (test) dataset.\n",
        "    '''\n",
        "    accuracy_numerator = 0\n",
        "    # Loop over the dataset we'd like to get the accuracy on\n",
        "    for i, point in enumerate(data):\n",
        "        # Compute the neighbors and classification for this point with the training data\n",
        "        classification = compute_neighbors_and_classify(X_train, y_train, point, k)\n",
        "        # Compare this classification to the real value in `labels`. \n",
        "        # If the model got it correct, add to a running sum of correct predictions.\n",
        "        accuracy_numerator += 1 if classification == labels[i] else 0\n",
        "    \n",
        "    # Compute the accuracy: divide the count of correct predictions by the number of samples in the dataset\n",
        "    accuracy = accuracy_numerator / len(labels)\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgoSkYMcAeRC"
      },
      "source": [
        "#### 3.d. Run KNN on the NIST dataset with k = 1, and report the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FdEHUl2AeRD",
        "outputId": "a797477c-0ae0-430a-b093-3deba54f5252"
      },
      "source": [
        "# Call compute_accuracy_on_dataset() to get the accuracy on the test set with k=1\n",
        "knn_acc = compute_accuracy_on_dataset(X_train, y_train, X_test, y_test, 1)\n",
        "print(\"Accuracy on test set with k = 1:\", knn_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set with k = 1: 0.9777777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdeLudA6AeRD"
      },
      "source": [
        "#### 3.d. Find the best value of K by using the validation set.\n",
        "Ideally, we'd like to programmatically find the best value of `k` instead of just guessing what it is. Using the **validation set**, find the best value of `k` by running our KNN algorithm for `k = [1, 2, 3 ... 10]`, and saving the `k` with the best accuracy. \n",
        "\n",
        "Fill in the function below, and then call it to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkw8dZcHAeRD"
      },
      "source": [
        "def validate_k_on_dataset(X_train, y_train, X_val, y_val):\n",
        "    '''\n",
        "    A function that finds the best K using a (validation) dataset.\n",
        "    '''\n",
        "    # Initialize the best accuracy and associated k\n",
        "    best_knn_acc = 0\n",
        "    best_k = 0\n",
        "    # In Python, range doesn't include the last element; this is the list [1,2,...10]\n",
        "    k_vec = list(range(1, 11))\n",
        "    # Loop through each k\n",
        "    for i in k_vec:\n",
        "        # Compute the accuracy on the validation set using the current k\n",
        "        knn_acc = compute_accuracy_on_dataset(X_train, y_train, X_val, y_val, i)\n",
        "        # If this k is better, replace the current best accuracy and k\n",
        "        if knn_acc > best_knn_acc:\n",
        "            best_knn_acc = knn_acc\n",
        "            best_k = i\n",
        "            \n",
        "    return best_knn_acc, best_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3p4X4wXAeRD",
        "outputId": "f9afed00-8f5d-4ce5-fcf3-5a6d4c14cefc"
      },
      "source": [
        "# Call the above function and print the resulting accuracy and best k for the validation set\n",
        "best_knn_acc, best_k = validate_k_on_dataset(X_train, y_train, X_val, y_val)\n",
        "print(\"Best accuracy on validation set was\", best_knn_acc, \"with k =\", best_k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best accuracy on validation set was 0.9944444444444445 with k = 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5WUJsN9AeRD"
      },
      "source": [
        "#### 3.e. Merge the training and validation sets, and report the new test accuracy using the best `k`.\n",
        "Since we've already used the validation set to obtain the best `k`, we can now merge the training set and the validation set and recompute KNN on the test set with the best `k`. \n",
        "\n",
        "Make sure to merge the data points in the same order that you merge the labels!\n",
        "\n",
        "##### Merge the training and validation sets. *(Hint*: Use [np.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html) and [np.concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ3_5V_LAeRD"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "# Use np.vstack to stack X_train and X_val\n",
        "X_val_and_train = np.vstack((X_val, X_train))\n",
        "\n",
        "# Use np.concatenate to concatenate y_train and y_val\n",
        "y_val_and_train = np.concatenate((y_val, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIQBgyrLAeRE"
      },
      "source": [
        "###### Now, recompute the accuracy on the test set using this merged dataset (instead of just the training set) and the best `k` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxeYSdc6AeRE",
        "outputId": "1f1d47d1-d925-466e-f891-9fe29f01920e"
      },
      "source": [
        "# Compute the accuracy on the test set using the merged train and validation set and the best k\n",
        "knn_acc = compute_accuracy_on_dataset(X_val_and_train, y_val_and_train, X_test, y_test, best_k)\n",
        "print(\"KNN accuracy on test set using merged train and validation sets and best K value:\", knn_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN accuracy on test set using merged train and validation sets and best K value: 0.9833333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33JOYsYRAeRE"
      },
      "source": [
        "### 4. Comparing Decision Trees and K-Nearest Neighbors on Raw Wine Data\n",
        "We're going to compare the performance of our KNN algorithm and the decision tree algorithm on a new dataset.\n",
        "\n",
        "The wine dataset is the result of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are 13 different measurements (features) taken for different constituents found in the three types of wine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKpwbau7AeRE"
      },
      "source": [
        "#### Import the wine dataset and view it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtR2yRA4AeRE",
        "outputId": "a160976d-5a85-474b-f3dc-e978427b3255"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Import the data into a pandas dataframe\n",
        "wine = load_wine()\n",
        "wine_df = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "\n",
        "# Split data into features and labels\n",
        "X_w = wine.data\n",
        "y_w = wine.target\n",
        "\n",
        "# View the raw data\n",
        "wine_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
              "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
              "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
              "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
              "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
              "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
              "\n",
              "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
              "0        3.06                  0.28             2.29             5.64  1.04   \n",
              "1        2.76                  0.26             1.28             4.38  1.05   \n",
              "2        3.24                  0.30             2.81             5.68  1.03   \n",
              "3        3.49                  0.24             2.18             7.80  0.86   \n",
              "4        2.69                  0.39             1.82             4.32  1.04   \n",
              "\n",
              "   od280/od315_of_diluted_wines  proline  \n",
              "0                          3.92   1065.0  \n",
              "1                          3.40   1050.0  \n",
              "2                          3.17   1185.0  \n",
              "3                          3.45   1480.0  \n",
              "4                          2.93    735.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGcEGquAeRE"
      },
      "source": [
        "#### 4.a. Split the data into train, test and validation sets.\n",
        "\n",
        "Use a 60/20/20 split. Make sure to set `random_state=42` to shuffle the dataset in a consistent manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QCJzXkoAeRF"
      },
      "source": [
        "# Split the data into training, test, and validation sets, just like we did on HW 2\n",
        "# Make sure to set random_state=42!\n",
        "X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(X_w, y_w, shuffle=True, random_state=42, test_size=0.2)\n",
        "X_w_train, X_w_val, y_w_train, y_w_val = train_test_split(X_w_train, y_w_train, shuffle=True, random_state=42, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S6OBUIFAeRF"
      },
      "source": [
        "#### 4.b. KNN: Find the best `k` and accuracy for this dataset.\n",
        "Use the same process as in question 3 to:\n",
        "\n",
        "##### 1. Validate `k`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGHXFKJRAeRF",
        "outputId": "870a4923-72e5-4a42-8e33-57cff74f5a62"
      },
      "source": [
        "# Call validate_k_on_dataset() and print the resulting accuracy and best k for the validation set\n",
        "best_w_knn_acc, best_w_k = validate_k_on_dataset(X_w_train, y_w_train, X_w_val, y_w_val)\n",
        "print(\"Best accuracy on validation set was\", best_w_knn_acc, \"with k =\", best_w_k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best accuracy on validation set was 0.8055555555555556 with k = 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61S6332pAeRF"
      },
      "source": [
        "##### 2. Merge the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d00SDez4AeRF"
      },
      "source": [
        "# Use np.vstack to stack X_train and X_val\n",
        "X_w_val_and_train = np.vstack((X_w_val, X_w_train))\n",
        "\n",
        "# Use np.concatenate to concatenate y_train and y_val\n",
        "y_w_val_and_train = np.concatenate((y_w_val, y_w_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5fCcfFJAeRF"
      },
      "source": [
        "##### 3. Report the new accuracy on the test set; recompute KNN using the merged dataset and the best `k` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbNUQWc_AeRG",
        "outputId": "c2d2e50b-8e01-485d-c48f-aa776265fef3"
      },
      "source": [
        "# Compute the accuracy on the test set using the merged train and validation set and the best k\n",
        "w_knn_acc = compute_accuracy_on_dataset(X_w_val_and_train, y_w_val_and_train, X_w_test, y_w_test, best_w_k)\n",
        "print(\"KNN accuracy on test set using merged train and validation sets and best K value:\", w_knn_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN accuracy on test set using merged train and validation sets and best K value: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoR0heCAeRG"
      },
      "source": [
        "#### 4.c. Decision Trees: Use `sklearn`'s built-in decision tree on this dataset and report the accuracy.\n",
        "You can find the documentation for instantiating and fitting `sklearn`'s `DecisionTreeClassifier` [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
        "\n",
        "You can find the documentation for using `metrics.accuracy_score` to compute the accuracy given predictions and true labels [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
        "\n",
        "*Make sure to use the merged training and validation set to fit the decision tree (for a fair comparison to KNN)!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjCRSBWkAeRG",
        "outputId": "9807b2bf-d2f1-4cd3-bb7c-0f0dde8a3456"
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn import metrics\n",
        "\n",
        "# Instantiate the decision tree.\n",
        "my_tree = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Fit the tree with the merged training and validation set.\n",
        "my_tree.fit(X_w_val_and_train, y_w_val_and_train)\n",
        "\n",
        "# Use the tree to predict on the test set.\n",
        "my_treepred = my_tree.predict(X_w_test)\n",
        "\n",
        "# Use metrics.accuracy_score to get the accuracy of this decision tree on the merged validation and testing set.\n",
        "tree_acc = metrics.accuracy_score(y_w_test, my_treepred)\n",
        "\n",
        "print(\"Decision tree accuracy on test set:\", tree_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision tree accuracy on test set: 0.9444444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GpSb3tGAeRG"
      },
      "source": [
        "#### 4.d. Explain the difference in performance between these two algorithms; why does one perform better than the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXBZGlHhAeRG"
      },
      "source": [
        "**Solution**\n",
        "\n",
        "KNN performs worse on the wine dataset than the decision tree. \n",
        "\n",
        "KNN performs better when the data is normalized (where the features have approximately the same scale). In the case that the data is not normalized, the features that have smaller values may become uninformative, and the algorithm will essentially rely on the dimension(s) whose values are substantially larger (due to the distance metric). \n",
        "\n",
        "In this case, the wine dataset has one particular feature (Proline) that is ~1000x larger than all the other features. This is the probable reason KNN performs poorly on this dataset as opposed to the NIST dataset - its distance metric does not accurately represent how different certain samples are, since it is relying largely on the Proline feature."
      ]
    }
  ]
}