{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_ih-R1fJ6VG"
      },
      "source": [
        "# Homework 4: Linear Binary Classification Methods\n",
        "by Junyu Liu and Brian Kulis\n",
        "\n",
        "**Due date**: March 3, Wednesday by 11:59pm\n",
        "\n",
        "**Late** due date: March 6, Saturday by 11:59pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxGJCzSKovR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mxor3LzKpqj"
      },
      "source": [
        "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\n",
        "\n",
        "To use Google Colab:\n",
        "\n",
        "1. Download this `ipynb` file.\n",
        "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\n",
        "3. Upload this file. It will then open in Colab.\n",
        "\n",
        "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\n",
        "\n",
        "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\n",
        "\n",
        "**Submission instructions:** please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by the due date (see Schedule)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTtN4g49nWrB"
      },
      "source": [
        "This homework is scored out of 100.\n",
        "\n",
        "\n",
        "**Important:** unless otherwise specified, you should **NOT** use loops. This is not to say loops are always bad, but avoiding them should help you:\n",
        "1.   get more familiar with common language features and libraries\n",
        "2.   write more efficient code\n",
        "3.   \"think in higher dimensions\" (get more comfortable with vectors, matrices, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32VwlhsN9ny"
      },
      "source": [
        "## **Question 1:** Creating a Dataset (10 points)\n",
        "\n",
        "For this assignment, we will create a simple linearly-separable dataset for binary classification. We have provided you with the code to generate the feature vectors. Notice that one class has significantly more samples than the other.\n",
        "\n",
        "**Important:** Although this dataset has only 1 feature, **ALL** the code you write in this assignment should be able to run as intended with more features. The only exception is where you are producing plots. Many functions you need to write will be tested for compatibility with more features in question 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS0rlO3eOCZK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import default_rng"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw5AUAsV5MQr"
      },
      "source": [
        "# Do NOT change\n",
        "rng = default_rng(1)\n",
        "x1 = rng.uniform(-4, 2, 460)\n",
        "x2 = rng.normal(5, np.sqrt(3), 40)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7rbmFam-xWg"
      },
      "source": [
        "### Problem a. (4 points)\n",
        "We need to create appropriate labels for the two classes. x1 is the feature vectors of the negative class and x2 the positive class. Also produce a colored feature-label scatter plot (the negative class should be blue and the positive class should be red)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1i19iWdkAVpY",
        "outputId": "4744b401-eec6-4d90-ceef-1538cdbeb5c0"
      },
      "source": [
        "# WRITE CODE HERE: \n",
        "y1 = np.zeros(460)-1\n",
        "y2 = np.ones(40)\n",
        "\n",
        "plt.scatter(x1, y1)\n",
        "plt.scatter(x2, y2, color='r')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fb10cfffa10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRLTGhczArfW"
      },
      "source": [
        "### Problem b. (3 points)\n",
        "Combine the data into X and y, and then perform a 75-25 train-test split. Use 32 as the random_state.\n",
        "\n",
        "Hint: X has to be a 2D array, so you need to use reshape at some point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofFs0MR9Dp7z"
      },
      "source": [
        "X = np.concatenate([x1, x2])\n",
        "X = X.reshape(-1, 1)\n",
        "y = np.concatenate([y1, y2])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=32, test_size=0.25)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlkYTs619A_"
      },
      "source": [
        "### Problem c. (3 points)\n",
        "To conveniently account for the bias term in later parts, we will also extend all feature vectors by appending a 1. Apply this to both X_train and X_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LFCgyVU3EKY"
      },
      "source": [
        "#WRITE CODE HERE:\n",
        "# Xtr_ext = 0 # X_train, extended\n",
        "# Xte_ext = 0 # X_test, extended\n",
        "ntr = X_train.shape[0]\n",
        "Xtr_ext = np.c_[X_train, np.ones(ntr)]\n",
        "\n",
        "nte = X_test.shape[0]\n",
        "Xte_ext = np.c_[X_test, np.ones(nte)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7z43OcK2ax"
      },
      "source": [
        "## **Question 2:** OLS Regression for Binary Classification (20 points)\r\n",
        "\r\n",
        "In class, we talked about one way to implement clasification is to use one of the linear regression methods we learned. We will investigate how to use OLS for binary classification and see how it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZX2J8wLVix"
      },
      "source": [
        "### Problem a. (3 points)\n",
        "\n",
        "When OLS takes a feature vector, it gives you a real-valued scalar. Assuming OLS works well (i.e., it gives a value close to the binary label (either -1 or 1) for most of the samples), how can you translate the value into the label? Suggest a simple method.\n",
        "\n",
        "Hint: we talked about this in class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdm7yHm72aa"
      },
      "source": [
        "We can use the sign function (numpy.sign) to translate the regression value into binary label. Ideally, we want the sign function to be defined as $sign(x) = 1$ if $x\\ge 0$, -1 otherwise, but numpy.sign is good enough for our case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEFD30vtfu3T"
      },
      "source": [
        "### Problem b. (7 points)\n",
        "For simplicity, we will use the OLS implementation from sklearn whose documentation can be found here:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "\n",
        "Fit the model to X_train, y_train and then extract w and b. You should do so by using get_wOLS_ext, which you need to complete. Note that the function should return a single vector, which is the regular w vector with the bias term appended to it.\n",
        "\n",
        "Lastly, create the following plot:\n",
        "* a scatter plot of the training data overlayed by the OLS solution. For this, You can assume that X_train has only one feature.\n",
        "\n",
        "Hint: w is the coeficient and b is the intercept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvgEoJUMcpEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "274de47d-8fc0-4a73-9d4d-4b9366389ba3"
      },
      "source": [
        "from sklearn import linear_model\r\n",
        "# WRITE CODE HERE: \r\n",
        "# DO NOT assume that X has only one feature\r\n",
        "def get_wOLS_ext(X, y):\r\n",
        "    ols=linear_model.LinearRegression()\r\n",
        "    ols.fit(X, y)\r\n",
        "    wOLS_ext = np.append(ols.coef_, ols.intercept_)\r\n",
        "    return wOLS_ext\r\n",
        "\r\n",
        "plt.scatter(X_train, y_train)\r\n",
        "wOLS_ext = get_wOLS_ext(X_train, y_train)\r\n",
        "plt.plot(X_train, Xtr_ext@wOLS_ext, color='r')\r\n",
        "print('wOLS_ext:', wOLS_ext)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wOLS_ext: [ 0.15042363 -0.76859499]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhgVuFW9w773"
      },
      "source": [
        "### Problem c. (6 points)\n",
        "\n",
        "Complete the linear_binary_predict function to predict the labels of the test set using the OLS solution from part a. Also complete compute_CCR and report the correct classification rate (CCR).\n",
        "\n",
        "linear_binary_predict takes Xext, a matrix of extended feature vectors, and wext, an extended weight vector; it performs linear binary classification and returns a vector of predicted binary labels.\n",
        "\n",
        "compute_CCR takes Xext, wext, along with y, the expected binary labels to compute CCR. It should make use of linear_binary_predict.\n",
        "\n",
        "Note: \n",
        "*   We assume Xext to be row-major, meaning each row is a sample.\n",
        "*   Do not change the function prototype of linear_binary_predict. This also applies to all other functions we ask you to complete\n",
        "*   The OLS from sklearn has a builtin predict function. We are asking you to complete linear_binary_predict instead of using the builtin predict because linear_binary_predict will be important for later parts.\n",
        "\n",
        "Hint: use the idea from part a.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrhLQ-0_x8OX",
        "outputId": "015f2bcf-6ca4-45bc-ee19-1224d0aead8c"
      },
      "source": [
        "def linear_binary_predict(Xext, wext):\n",
        "    return np.sign(Xext @ wext)\n",
        "\n",
        "def compute_CCR(Xext, wext, y):\n",
        "    return np.mean(linear_binary_predict(Xext, wext) == y)\n",
        "\n",
        "CCR = compute_CCR(Xte_ext, wOLS_ext, y_test)\n",
        "print(\"The test CCR using OLS is\", CCR)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using OLS is 0.928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX-NCA9sZ5XO"
      },
      "source": [
        "### Problem d. (4 points)\n",
        "Explain why CCR is not a good metric in this case. Suggest an alternative that better captures the performance of OLS on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZmVPImh-XSk"
      },
      "source": [
        "0.928 looks decent at first glance. However, the model actually classifies the majority of the positive class as negative. If this were a medical test where detecting positives is the primary goal, the performance should be considered horrible. An alternative is to use a confusion matrix, which can show how the model performs on each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF7u7IoGC96w"
      },
      "source": [
        "## **Question 3:** Fisher's Linear Discriminant (27 points)\n",
        "Fisher's Linear Discriminant is a method that takes d dimensional feature vectors and projects them into 1 dimension, and it tries to do so in a fashion where the resulting 1D values are well-separated by class. Here our d happens to be 1, but as mentioned above, the code you write needs to be able to accomodate higher dimensions for full credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaVLlwwJWrpX"
      },
      "source": [
        "### a. Separate the training set by class (4 points)\n",
        "\n",
        "Write a function seperate that takes the inputs X, y and separates X based on y.\n",
        "\n",
        "For full credit, create a vectorized implementation (no for loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991ZOOYCY02l"
      },
      "source": [
        "def seperate(X, y):\n",
        "    # WRITE CODE HERE: \n",
        "    X1 = X[y<0]\n",
        "    X2 = X[y>=0]\n",
        "    # X1 should be of the negative class, and X2 the positive class\n",
        "    return X1, X2\n",
        "\n",
        "X1, X2 = seperate(X_train, y_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMMqIr0xB9BP"
      },
      "source": [
        "### b. Calculate mean vectors (4 points)\n",
        "Write a function get_means that takes the inputs X1, X2 and calculates the mean vectors of the two classes.\n",
        "\n",
        "For full credit, create a vectorized implementation (no for loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuPgvLQKTPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1711acd4-4a8d-46f6-a0e9-22c7b6b87a92"
      },
      "source": [
        "def get_means(X1, X2):\n",
        "    # WRITE CODE HERE: \n",
        "    m1 = np.mean(X1, axis=0)\n",
        "    m2 = np.mean(X2, axis=0)\n",
        "\n",
        "    return m1, m2 \n",
        "\n",
        "m1, m2 = get_means(X1, X2)\n",
        "print('m1, m2:', m1, m2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m1, m2: [-1.07085432] [5.73185771]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3xQqSq_Ogmd"
      },
      "source": [
        "### c. Calculate within-class covariance (5 points)\n",
        "$S_w$, the total within-class covariance matrix, is a d by d matrix given by $S_1 + S_2$ where $S_1 = \\sum_{x_i \\in X_1}(x_i-m_1)^T(x_i-m_1) = (X_1-m_1)^T(X_1-m_1)$. \n",
        "\n",
        "Write a function get_Sw that takes the inputs X1, X2, m1, m2 and calculates Sw.\n",
        "\n",
        "For full credit, create a vectorized implementation.\n",
        "\n",
        "**Note:** depending on the schema of the dataset (row-major vs column-major), the formula to use might differ slightly from those on the lecture slides. We assume row-major (each sample is a row) here, as many public datasets are organized this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5NaX0uwUXll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5eaa731-55dc-4235-f6e9-8120d7b70dc1"
      },
      "source": [
        "def get_Sw(X1, X2, m1, m2):\n",
        "    dev1 = X1-m1 # deviations in class 1; can also think of it as zero-centering\n",
        "    S1 = dev1.T @ dev1\n",
        "    dev2 = X2-m2\n",
        "    S2 = dev2.T @ dev2\n",
        "    Sw = S1 + S2\n",
        "    return Sw\n",
        "\n",
        "Sw = get_Sw(X1, X2, m1, m2)\n",
        "print('Sw:', Sw)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sw: [[1106.73594709]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnV8WWDBdZAG"
      },
      "source": [
        "### d. Calculate $w_{FLD}$ and $b_{FLD}$ (5 points)\n",
        "Write a function get_wFLD_ext that takes the inputs Sw, m1, m2 and calculates the extended $w_{FLD}$.\n",
        "\n",
        "Recall that in class we mentioned the average of the averages of the two classes after the linear transformation can serve as the bias term. However, since we want the outputs of $Xw + b$ on the two classes to be roughly separated by 0, we actually want the effect of adding b to be the same as subtracting that average. Thus, use $b = -(m_1w + m_2w)/2$.\n",
        "\n",
        "For full credit, create a vectorized implementation, and do not use the inverse function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1BuMyNvdXm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51dfa60-5705-49bf-9091-48140572d7fa"
      },
      "source": [
        "def get_wFLD_ext(Sw, m1, m2):\n",
        "    # WRITE CODE HERE: \n",
        "    wFLD = np.linalg.solve(Sw, m2-m1)\n",
        "    bFLD = -(np.dot(wFLD, m1) + np.dot(wFLD, m2))/2\n",
        "    wFLD_ext = np.append(wFLD, bFLD)\n",
        "    return wFLD_ext\n",
        "\n",
        "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
        "print('wFLD_ext:', wFLD_ext)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wFLD_ext: [ 0.00614664 -0.01432476]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5FyLF32e9t0"
      },
      "source": [
        "### e. Evaluation (4 points)\n",
        "\n",
        "First, create a plot of the FLD solution using the training set. Then use the compute_CCR function that you wrote earlier to get the FLD CCR on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6E8A6SAwhTzy",
        "outputId": "b27b61dd-ee8a-42d2-f77c-e376fbdc9d7c"
      },
      "source": [
        "# WRITE CODE HERE: \n",
        "plt.plot(X_train, Xtr_ext@wFLD_ext)\n",
        "\n",
        "CCR = compute_CCR(Xte_ext, wFLD_ext, y_test)\n",
        "print(\"The test CCR using FLD is\", CCR)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using FLD is 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kG8hSp7WHCD"
      },
      "source": [
        "### f. Reflection (5 points)\n",
        "Recall that in class we noted that OLS and FLD can be equivalent under certain situations. Is that the case here? If not, briefly comment on why they performed differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa2X0WzPCFKw"
      },
      "source": [
        "No, FLD performs much better than OLS in this case. The positive class is severely underrepresented in the dataset, so it affects the OLS solution much less than the negative class does; on the other hand, FLD actively tries to give both classes equal amount of importance. This is clear in the derivation of these two methods: the loss function of OLS does not take classes into account, while FLD is tailored to maximize the class separation to noise ratio.\n",
        "\n",
        "As mentioned in the Bishop book, OLS can be artificially guided to behave exactly like FLD by adjusting the class labels (targets). Specifically, the negative label should be $\\frac{-N}{N_-}$, and the positive label should be $\\frac{N}{N_+}$. Intuitively, this adjusts the weight of each sample so that OLS adapts to the two classes more equally, just like in FLD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hte8xdEpZNZ"
      },
      "source": [
        "## **Question 4:** An algorithm using SGD (33 points)\n",
        "Let us consider a different linear classifier in this problem, one based on the idea of an $\\textit{error}$ function minimization problem, similar to what we studied with linear regression, but whose error function is more tailored to the classification problem.\n",
        "\n",
        "The form of our linear classifier is $h_{\\bf{w}}(\\bf{x}) = \\mbox{sgn}(\\bf{w}^T \\bf{x})$, where $sgn$ is the sign function (so $1$ if $\\bf{w}^T \\bf{x} \\geq 0$ and $-1$ otherwise).  We are assuming here that we have padded the inputs with an extra dimension of $1$ to account for the bias term in the linear function, as we did with linear regression.\n",
        "\n",
        "In the binary classification case, each target class $y_i$ is either $1$ or $-1$.  One possible error function to use would say that the error is 0 if our linear classifier agrees with the target label (i.e. $y_i = \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), and if our linear classifier predicts the incorrect class ($y_i \\neq \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), then the error will be given by $|\\bf{w}^T \\bf{x}_i|$---intuitively, we penalize more for predictions that are farther from being predicted correctly.\n",
        "\n",
        "**Note:** the analytical parts of this question assume column feature vectors. However, you should still assume row feature vectors in the coding parts for consistency with our datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19cL4fQjsHHC"
      },
      "source": [
        "### Problem a. (5 points)\n",
        "Show that, for each training point, this error function may be concisely written as\n",
        "\n",
        "$L_i(\\bf{w}) = \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJyt3ZrBuG2"
      },
      "source": [
        "If there is no error, then $y_i \\bf{w}^T \\bf{x}_i > 0$, since $y_i$ and $\\bf{w}^T \\bf{x}_i$ are either both positive or both negative.  If there is an error, then they have different signs, so $y_i \\bf{w}^T \\bf{x}_i < 0$.  Equivalently, $-y_i \\bf{w}^T \\bf{x}_i > 0$ if our classifier makes an error.  Since $y_i$ is either $1$ or $-1$, then $-y_i \\bf{w}^T \\bf{x}_i = |\\bf{w}^T \\bf{x}_i|$ in this case.  Looking at $L_i$, if there is no error, then the loss returns 0 as desired since $-y_i \\bf{w}^T \\bf{x}_i < 0$ and so $\\max(0, -y_i \\bf{w}^T \\bf{x}_i) = 0$.  If there is an error, the loss returns $|\\bf{w}^T \\bf{x}_i|$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlqWT0CXs5vk"
      },
      "source": [
        "### Problem b. (5 points)\n",
        "We will compute the total loss over the training data as the sum of the $L_i$ losses, namely\n",
        "\n",
        "$L(\\bf{w}) = \\sum_{i=1}^n \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$.\n",
        "\n",
        "Note that one reason we chose this particular loss function is that it is continuous (unlike the 0-1 loss).  Furthermore, it has a simple sub-gradient.  The sub-gradient of the loss at 0 is 0, and everywhere else it is equal to the gradient.  Show that the sub-gradient is equal to\n",
        "$\\nabla_{\\bf{w}} L_i = \n",
        "\\begin{cases}\n",
        "0 & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ correctly}\\\\\n",
        "-y_i \\bf{x}_i & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ incorrectly.}\n",
        "\\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQ26ILVB8xY"
      },
      "source": [
        "Since the loss is 0 whenever the classifier predicts correctly, the gradient of 0 is 0.  When there is an error, the error function pays $-y_i \\bf{w}^T \\bf{x}_i$, whose gradient with respect to $\\bf{w}$ is simply $-y_i \\bf{x}_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF24kNfQuSKB"
      },
      "source": [
        "### Problem c. (5 points)\n",
        "Using a batch size of 1 and a step size of 1, write down the update rule for stochastic (sub)-gradient descent to minimize $L(\\bf{w})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BZZldGACCr_"
      },
      "source": [
        "At each iteration, we use a batch size of 1, so we pick a single data point $\\bf{x}_i$ for the update.  The rule is that $\\bf{w}_{t+1} = \\bf{w}_t$ if the current classifier predicts correctly ($y_i = \\mbox{sgn}(\\bf{w}_t^T \\bf{x}_i)$).  Otherwise, we update via $\\bf{w}_{t+1} = \\bf{w}_t + y_i \\bf{x}_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOT7Z6BrV6T"
      },
      "source": [
        "### Problem d. (12 points)\n",
        "Now it's time to implement the actual algorithm using the update rule we derived from part c. \n",
        "\n",
        "1.   Complete the one_pass function, which iterates over the entire Xext and y once, updating w in the process. (6 points)\n",
        "2.   Go over the training set twice (2 epochs) to get your final w, which you can do by calling one_pass twice in the get_wSGD function. Then report the CCR on the test set. (6 points)\n",
        "\n",
        "Note:\n",
        "* You should always shuffle the training set before each pass\n",
        "* The first shuffle has already been performed by train_test_split, so a shuffle before the first pass is not required here.\n",
        "* If you run just this cell multiple times, you will likely get different CCRs because of shuflling. You should do a \"Run all\" before submitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14KZEtjvOuO",
        "outputId": "de6dd3b2-c43d-44a5-d11c-b8414958eafe"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def one_pass(Xext, y, w):\n",
        "    n = Xext.shape[0]\n",
        "    for i in range(n):\n",
        "        xi = Xext[i]\n",
        "        yi = y[i]\n",
        "        if yi*(xi@w) <= 0:\n",
        "            w += yi*xi\n",
        "\n",
        "def get_wSGD(Xext, y):\n",
        "    random_state=0\n",
        "\n",
        "    w = np.zeros(Xext.shape[1])\n",
        "    one_pass(Xext, y, w)\n",
        "    Xext, y = shuffle(Xext, y, random_state=random_state)\n",
        "    one_pass(Xext, y, w)\n",
        "    return w\n",
        "\n",
        "wSGD = get_wSGD(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD, y_test)\n",
        "print(\"The test CCR using SGD is\", CCR)\n",
        "print('wSGD:', wSGD)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using SGD is 0.92\n",
            "wSGD: [ 4.48944846 -5.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAyX-08Z1f5h"
      },
      "source": [
        "### Problem e. (6 points)\n",
        "\n",
        "The result you get from the previous part is likely not ideal. And if you change the random_state used for shuffling in the previous part (remember to change it back to 0 if you try this), you should see large swings in CCR. This is a common phenomenon when using SGD naively, and we will see this again in question 5.\n",
        "\n",
        "Recall from part c that we are using a fixed step size of 1, which is way too large for you to converge to a local minimum consistently. \n",
        "\n",
        "In practice, a technique called learning rate decay is commonly used with SGD. Essentially, the step size gets smaller after each iteration. This allows you to \"learn fast\" in the beginning and also be able to converge to a local minimum later when you are close to one.\n",
        "\n",
        "We will use an initial learning rate (step size) of 1 and the inverse square root decay, which means the step size at iteration $t$ is $\\frac1{\\sqrt t}$.\n",
        "\n",
        "Note:\n",
        "* $t$ starts at 1, not 0; otherwise, you will get a division by 0\n",
        "* You can get rid of the one_pass2 function if you think it's better to incorporate it into get_wSGD2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlM2SdxOvGvi",
        "outputId": "e4219ec3-99c3-45cd-90f6-c533462dc1ef"
      },
      "source": [
        "def one_pass2(Xext, y, w, t):\n",
        "    n = Xext.shape[0]\n",
        "    for i in range(n):\n",
        "        xi = Xext[i]\n",
        "        yi = y[i]\n",
        "        if yi*(xi@w) <= 0:\n",
        "            w += (yi*xi).T / np.sqrt(t)\n",
        "        t += 1\n",
        "\n",
        "def get_wSGD2(Xext, y):\n",
        "    random_state=0\n",
        "\n",
        "    w = np.zeros(Xext.shape[1])\n",
        "    one_pass2(Xext, y, w, 1)\n",
        "    Xext, y = shuffle(Xext, y, random_state=random_state)\n",
        "    one_pass2(Xext, y, w, len(y)+1)\n",
        "    return w\n",
        "\n",
        "wSGD2 = get_wSGD2(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD2, y_test)\n",
        "print(\"The test CCR using SGD with decay is\", CCR)\n",
        "print('wSGD2:', wSGD2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using SGD with decay is 1.0\n",
            "wSGD2: [ 0.92169208 -2.0213996 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcnj3HYJHTqa"
      },
      "source": [
        "## **Question 5:** Testing the above methods on a real-world dataset (10 points)\n",
        "No code is required for this part. Getting part a to run correctly is not required for part b.\n",
        "\n",
        "The purpose of this question is to compare the three (four if you count SGD with decay as a separate one) methods on a real-world dataset. We will use the famous Iris dataset created by Sir Ronald Fisher (the same Fisher as in Fisher's Linear Discriminant). It contains feature measurements of 150 samples from three *Iris* flower species. We will combine two of the classes into a single negative class and the remaining class is the positive class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekpBYEcfB8-3"
      },
      "source": [
        "### Problem a. (6 points)\n",
        "Run the cells below. The code for this part is provided to you, but it requires several earlier functions to be properly implemented to run. They need to be able to handle data with multiple features.\n",
        "\n",
        "You will be graded on how well the behavior of this part matches our expectation. Do NOT modify the code provided here; if you have issues running it, check the code you wrote in previous parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usHZVolZXXen"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into feature vectors and labels\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# change the label values for binary classification\n",
        "y[:100] = -1\n",
        "y[100:] = 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yfPgky0OvO-"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.25)\n",
        "Xtr_ext = np.c_[X_train, np.ones(X_train.shape[0])]\n",
        "Xte_ext = np.c_[X_test, np.ones(X_test.shape[0])]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9coGwIiKQZPh",
        "outputId": "769b69e1-9041-430b-e733-86fbcb060e7c"
      },
      "source": [
        "wOLS = get_wOLS_ext(X_train, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wOLS, y_test)\n",
        "print(\"The test CCR using OLS is\", CCR)\n",
        "\n",
        "X1, X2 = seperate(X_train, y_train)\n",
        "m1, m2 = get_means(X1, X2)\n",
        "Sw = get_Sw(X1, X2, m1, m2)\n",
        "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
        "CCR = compute_CCR(Xte_ext, wFLD_ext, y_test)\n",
        "print(\"The test CCR using FLD is\", CCR)\n",
        "\n",
        "wSGD = get_wSGD(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD, y_test)\n",
        "print(\"The test CCR using SGD is\", CCR)\n",
        "\n",
        "wSGD2 = get_wSGD2(Xtr_ext, y_train)\n",
        "CCR = compute_CCR(Xte_ext, wSGD2, y_test)\n",
        "print(\"The test CCR using SGD with decay is\", CCR)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test CCR using OLS is 0.8947368421052632\n",
            "The test CCR using FLD is 0.8947368421052632\n",
            "The test CCR using SGD is 0.6052631578947368\n",
            "The test CCR using SGD with decay is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxpckb9mCE_P"
      },
      "source": [
        "### Problem b. (4 points)\n",
        "A class containing sub-classes is quite common in real-world classifications. One direct effect of this is that within such a class, the feature vectors tend to be quite spread out (in distinct clusters). Comment on the effect this may have on OLS and FLD.\n",
        "\n",
        "Hint: If the previous part runs correctly, you should see OLS and FLD perform worse than the SGD algorithm from problem 4e. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e71oQW2IFCt7"
      },
      "source": [
        "For OLS, a class that has sub-classes/clusters with distinct features compels the least squares loss function to drive w towards 0 to fit the clusters with different features but the same label value. You can also think of it as within-class clusters indicating weak linear relationship between the features and the label.\n",
        "\n",
        "For FLD, a class with highly spread-out features has a high within-class covariance. In a sense, because the within-class covariance is so high, too much attention is given to reducing it rather than separating the two classes. Also within-class clusters break the assumption of multivariate normality, so it's no surprise that FLD doesn't work well here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zlg5JfViya"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}